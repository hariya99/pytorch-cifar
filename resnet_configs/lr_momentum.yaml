
lr05mom025:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.025
  lr: 0.05 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr05mom05:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.05
  lr: 0.05 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr05mom1:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.1
  lr: 0.05 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr05mom2:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.2
  lr: 0.05 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr05mom4:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.4
  lr: 0.05 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr05mom8:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.8
  lr: 0.05 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr1mom025:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.025
  lr: 0.1 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr1mom05:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.05
  lr: 0.1 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr1mom1:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.1
  lr: 0.1 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr1mom2:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.2
  lr: 0.1 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr1mom4:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.4
  lr: 0.1 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr1mom8:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.8
  lr: 0.1 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr2mom025:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.025
  lr: 0.2 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr2mom05:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.05
  lr: 0.2 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr2mom1:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.1
  lr: 0.2 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr2mom2:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.2
  lr: 0.2 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr2mom4:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.4
  lr: 0.2 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr2mom8:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.8
  lr: 0.2 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr4mom025:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.025
  lr: 0.4 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr4mom05:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.05
  lr: 0.4 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr4mom1:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.1
  lr: 0.4 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr4mom2:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.2
  lr: 0.4 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr4mom4:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.4
  lr: 0.4 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr4mom8:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.8
  lr: 0.4 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr8mom025:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.025
  lr: 0.8 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr8mom05:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.05
  lr: 0.8 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr8mom1:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.1
  lr: 0.8 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr8mom2:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.2
  lr: 0.8 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr8mom4:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.4
  lr: 0.8 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr8mom8:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.8
  lr: 0.8 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1
