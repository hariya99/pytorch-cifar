b64lr05:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.05 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

b128lr05:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.05 
  weight_decay: 0.0005 
  batch_size: 128
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

b256lr05:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.05 
  weight_decay: 0.0005 
  batch_size: 256
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

b512lr05:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.05 
  weight_decay: 0.0005 
  batch_size: 512
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

b64lr1:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.1 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

b128lr1:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.1 
  weight_decay: 0.0005 
  batch_size: 128
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

b256lr1:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.1 
  weight_decay: 0.0005 
  batch_size: 256
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

b512lr1:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.1 
  weight_decay: 0.0005 
  batch_size: 512
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

b64lr2:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.2 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

b128lr2:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.2
  weight_decay: 0.0005 
  batch_size: 128
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

b256lr2:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.2 
  weight_decay: 0.0005 
  batch_size: 256
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

b512lr2:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.2 
  weight_decay: 0.0005 
  batch_size: 512
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1
--
b64lr4:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.4 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

b128lr4:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.4
  weight_decay: 0.0005 
  batch_size: 128
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

b256lr4:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.4 
  weight_decay: 0.0005 
  batch_size: 256
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

b512lr4:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.4 
  weight_decay: 0.0005 
  batch_size: 512
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

b64lr8:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.8 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

b128lr8:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.8
  weight_decay: 0.0005 
  batch_size: 128
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

b256lr8:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.8 
  weight_decay: 0.0005 
  batch_size: 256
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

b512lr8:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.8 
  weight_decay: 0.0005 
  batch_size: 512
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1
