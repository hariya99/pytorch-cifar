
lr05batch64:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.05 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr05batch128:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.05 
  weight_decay: 0.0005 
  batch_size: 128
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr05batch256:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.05 
  weight_decay: 0.0005 
  batch_size: 256
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr05batch512:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.05 
  weight_decay: 0.0005 
  batch_size: 512
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr1batch64:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.1 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr1batch128:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.1 
  weight_decay: 0.0005 
  batch_size: 128
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr1batch256:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.1 
  weight_decay: 0.0005 
  batch_size: 256
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr1batch512:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.1 
  weight_decay: 0.0005 
  batch_size: 512
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr2batch64:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.2 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr2batch128:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.2 
  weight_decay: 0.0005 
  batch_size: 128
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr2batch256:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.2 
  weight_decay: 0.0005 
  batch_size: 256
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr2batch512:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.2 
  weight_decay: 0.0005 
  batch_size: 512
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr4batch64:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.4 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr4batch128:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.4 
  weight_decay: 0.0005 
  batch_size: 128
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr4batch256:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.4 
  weight_decay: 0.0005 
  batch_size: 256
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr4batch512:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.4 
  weight_decay: 0.0005 
  batch_size: 512
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr8batch64:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.8 
  weight_decay: 0.0005 
  batch_size: 64
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr8batch128:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.8 
  weight_decay: 0.0005 
  batch_size: 128
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr8batch256:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.8 
  weight_decay: 0.0005 
  batch_size: 256
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1

lr8batch512:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3,3,3,3] 
  num_blocks: [2,1,1,1] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.8 
  weight_decay: 0.0005 
  batch_size: 512
  num_workers: 2
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1
